{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3723297,"sourceType":"datasetVersion","datasetId":2222120},{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565},{"sourceId":9177835,"sourceType":"datasetVersion","datasetId":5546853},{"sourceId":9941146,"sourceType":"datasetVersion","datasetId":6112205},{"sourceId":9947105,"sourceType":"datasetVersion","datasetId":6116647}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install important library","metadata":{}},{"cell_type":"code","source":"pip install rouge-score sacrebleu evaluate torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:50:38.603802Z","iopub.execute_input":"2024-11-28T17:50:38.604144Z","iopub.status.idle":"2024-11-28T17:50:50.503412Z","shell.execute_reply.started":"2024-11-28T17:50:38.604109Z","shell.execute_reply":"2024-11-28T17:50:50.502488Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=12e2bbfd842d1fe3ea077fc608f62b6910aace48dce52571381130b8b4d4fe53\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: torchsummary, portalocker, sacrebleu, rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 portalocker-3.0.0 rouge-score-0.1.2 sacrebleu-2.4.3 torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport evaluate\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom transformers import BertTokenizer, BertModel, EncoderDecoderModel, Seq2SeqTrainingArguments\nfrom transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback, T5Config\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nfrom torch.utils.data import TensorDataset\nfrom torchsummary import summary\n\nfrom collections import defaultdict\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-28T17:50:50.505169Z","iopub.execute_input":"2024-11-28T17:50:50.505473Z","iopub.status.idle":"2024-11-28T17:51:08.139598Z","shell.execute_reply.started":"2024-11-28T17:50:50.505445Z","shell.execute_reply":"2024-11-28T17:51:08.138677Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"code","source":"#Load Data\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\n# Sampel Data\nprint(\"Data Sample\")\nprint(df.head())\n\n#Null value\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\nduplicates = df.duplicated(['question'], keep=False).sum()\nprint(f\"Total duplicates in 'question' column: {duplicates}\")\n\n# Check for duplicate rows\n\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Reset the index after removing duplicates\ndf.reset_index(drop=True, inplace=True)\n\n#Delete Unused column\ndf = df.drop(columns=['source', 'focus_area'])\n\n#Table Info\nprint(\"Table Info\")\nprint(df.info())\n\n# Apply the function\ndf = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\ndf = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n\n#Drop rows with null values\ndf.dropna(inplace=True)\n\n#Checking again of null values\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n#Checking again of the data info\nprint(df.info())\n\n#Check for Unique Data\nprint(f\"Unique questions: {df['question'].nunique()}\")\nprint(f\"Unique answers: {df['answer'].nunique()}\")\n\n# Removing \"(are)\" in the dataset\ndef clean_questions(question):\n    question = re.sub(r\"\\(.*?\\)\", \"\", question)\n    return question\ndf['question'] = df['question'].apply(clean_questions)\n\ndf['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\ndf['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-28T17:51:08.140970Z","iopub.execute_input":"2024-11-28T17:51:08.141483Z","iopub.status.idle":"2024-11-28T17:51:09.656683Z","shell.execute_reply.started":"2024-11-28T17:51:08.141454Z","shell.execute_reply":"2024-11-28T17:51:09.655670Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  \nNull Value Data\nquestion       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64\nTotal duplicates in 'question' column: 2319\nNumber of duplicate rows: 48\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16364 entries, 0 to 16363\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  16364 non-null  object\n 1   answer    16359 non-null  object\ndtypes: object(2)\nmemory usage: 255.8+ KB\nNone\nNull Value Data\nquestion    0\nanswer      0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nIndex: 14463 entries, 0 to 14463\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  14463 non-null  object\n 1   answer    14463 non-null  object\ndtypes: object(2)\nmemory usage: 339.0+ KB\nNone\nUnique questions: 14463\nUnique answers: 14463\n                                 question  \\\n0                      what is glaucoma ?   \n1                  what causes glaucoma ?   \n2     what are the symptoms of glaucoma ?   \n3  what are the treatments for glaucoma ?   \n4          who is at risk for glaucoma? ?   \n\n                                              answer  \n0  glaucoma is a group of diseases that can damag...  \n1  nearly 2.7 million people have glaucoma, a lea...  \n2  symptoms of glaucoma glaucoma can develop in o...  \n3  although open-angle glaucoma cannot be cured, ...  \n4  anyone can develop glaucoma. some people are a...  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Architecting Model","metadata":{}},{"cell_type":"code","source":"# Step 1: Load BioBERT tokenizer and model\nencoder_name = \"dmis-lab/biobert-base-cased-v1.1\"\ndecoder_name = \"bert-base-cased\"  \n\n# Load Tokenizers for both Encoder and Decoder\ntokenizer = BertTokenizer.from_pretrained(encoder_name)\ndecoder_tokenizer = BertTokenizer.from_pretrained(decoder_name)\n\n# Define Encoder-Decoder model\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_name, decoder_name)\n\n# Tie weights between encoder and decoder embeddings (optional)\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.min_length = 3\nmodel.config.no_repeat_ngram_size = 2\nmodel.config.length_penalty = 2.0\nmodel.config.early_stopping = True\n\n# Enable dropout in both encoder and decoder for regularization\nmodel.encoder.config.hidden_dropout_prob = 0.2\nmodel.encoder.config.attention_probs_dropout_prob = 0.2\nmodel.decoder.config.hidden_dropout_prob = 0.2\nmodel.decoder.config.attention_probs_dropout_prob = 0.2\n\n# Print the model architecture\ndef summarize_model_by_type(model):\n    layer_summary = defaultdict(int)\n    param_summary = defaultdict(int)\n\n    for name, module in model.named_modules():\n        layer_type = type(module).__name__\n        layer_summary[layer_type] += 1\n        param_summary[layer_type] += sum(p.numel() for p in module.parameters())\n\n    print(f\"{'Layer Type':<30}{'Count':<10}{'Parameters':<15}\")\n    print(\"=\" * 55)\n    for layer_type, count in layer_summary.items():\n        print(f\"{layer_type:<30}{count:<10}{param_summary[layer_type]:<15,}\")\n\nsummarize_model_by_type(model)\n\n# Step 2: Preprocess function for seq2seq\ndef preprocess_function(batch):\n    inputs = [f\"question: {q}\" for q in batch['question']]\n    targets = [f\"{a}\" for a in batch['answer']]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        inputs,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = decoder_tokenizer(\n            targets,\n            max_length=64,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    labels[\"input_ids\"][labels[\"input_ids\"] == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Step 3: Train-test split\ntrain_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Preprocess datasets\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,  \n    remove_columns=train_dataset.column_names,\n    num_proc=4,   \n)\n\nval_dataset = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,  \n    remove_columns=val_dataset.column_names,\n    num_proc=4,  \n)\n\n# Step 4: Define TrainingArguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=1000,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    num_train_epochs=10,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\",\n)\n\n# Step 5: Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    padding='longest',\n)\n\n# Step 6: Define compute_metrics\ndef compute_metrics(eval_pred):\n    # Unpack predictions and labels\n    predictions, labels = eval_pred\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n\n    # Decode predictions\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n\n    # Replace -100 in labels with pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Normalize text\n    def normalize_text(text):\n        text = text.strip().lower()\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        return text\n\n    decoded_preds = [normalize_text(pred) for pred in decoded_preds]\n    decoded_labels = [normalize_text(label) for label in decoded_labels]\n\n    # Compute exact match\n    exact_matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n    exact_match_accuracy = np.mean(exact_matches)\n\n    return {\"exact_match\": exact_match_accuracy}\n\n# Step 7: Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Step 8: Train and Save Model\ntrainer.train()\ntrainer.save_model(\"./biobert_seq2seq_model\")\ntokenizer.save_pretrained(\"./biobert_seq2seq_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-11-28T17:51:09.659370Z","iopub.execute_input":"2024-11-28T17:51:09.659875Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674ff54d69ac4f0186671e95b024d262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9d1e90ff79f4c09a40960feba7f7e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764daf2d2df94c5894875c42a5bcbf75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36d6d2395454763ba49efb0d98d6671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8f1eb6917a4e7e93a79a78eecf568c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a008148eaf94b908a2a122d4d80fb7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446394afac1049b4976ec8d5fb87d46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152c7dcfe240434b8e47723ba4c06f7d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"name":"stdout","text":"Layer Type                    Count     Parameters     \n=======================================================\nEncoderDecoderModel           1         245,017,924    \nBertModel                     2         244,396,800    \nBertEmbeddings                2         45,330,432     \nEmbedding                     6         45,327,360     \nLayerNorm                     63        96,768         \nDropout                       98        0              \nBertEncoder                   2         198,475,776    \nModuleList                    2         198,475,776    \nBertLayer                     24        198,475,776    \nBertAttention                 36        85,100,544     \nBertSdpaSelfAttention         36        63,783,936     \nLinear                        195       221,862,724    \nBertSelfOutput                36        21,316,608     \nBertIntermediate              24        56,696,832     \nGELUActivation                25        0              \nBertOutput                    24        56,678,400     \nBertPooler                    1         590,592        \nTanh                          1         0              \nBertLMHeadModel               1         136,707,652    \nBertOnlyMLMHead               1         22,890,052     \nBertLMPredictionHead          1         22,890,052     \nBertPredictionHeadTransform   1         592,128        \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/12293 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c39e4b9e407471c812e32fd75ba6179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2170 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d3536eb04b4852921274a1aaa39487"}},"metadata":{}},{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='543' max='7690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 543/7690 08:37 < 1:53:55, 1.05 it/s, Epoch 0.70/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.569100</td>\n      <td>2.366683</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Show graphics","metadata":{}},{"cell_type":"code","source":"# Extract loss values from log history\ntrain_loss = []\neval_loss = []\neval_bleu = []\neval_exact_match = []\neval_rogue = []\nsteps = []\neval_steps = []\n\nfor log in log_history:\n    if \"loss\" in log:\n        train_loss.append(log[\"loss\"])\n        steps.append(log[\"step\"])\n    if \"eval_loss\" in log:\n        eval_loss.append(log[\"eval_loss\"])\n        eval_steps.append(log[\"step\"])\n    if \"eval_BLEU\" in log:\n        eval_bleu.append(log[\"eval_BLEU\"])\n    if \"eval_ROUGE-L\" in log:\n        eval_bleu.append(log[\"eval_ROUGE-L\"])\n    if \"eval_exact_match\" in log:\n        eval_exact_match.append(log[\"eval_exact_match\"])\n\n# Plot the losses\nplt.figure(figsize=(10, 6))\nplt.plot(steps, train_loss, label=\"Training Loss\", color=\"blue\", marker=\"o\")\nplt.plot(steps[:len(eval_loss)], eval_loss, label=\"Evaluation Loss\", color=\"orange\", marker=\"o\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Evaluation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the BLEU\nplt.figure(figsize=(10, 6))\nplt.plot(eval_bleu, label=\"BLEU\", marker=\"o\", linestyle=\"-\", color=\"green\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Metric Score\")\nplt.title(\"BLEU Score Over Training Steps\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Plot the ROGUE\nplt.figure(figsize=(10, 6))\nplt.plot(eval_bleu, label=\"ROGUE-L\", marker=\"o\", linestyle=\"-\", color=\"red\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Metric Score\")\nplt.title(\"ROGUE-L Score Over Training Steps\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Plot the BLEU\nplt.figure(figsize=(10, 6))\nplt.plot(eval_bleu, label=\"Exact Match\", marker=\"o\", linestyle=\"-\", color=\"black\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Metric Score\")\nplt.title(\"Exact match Over Training Steps\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing using Input","metadata":{}},{"cell_type":"code","source":"# Load the trained T5 model and tokenizer\nmodel_path = \"/kaggle/working/t5_chatbot_model\"\ntokenizer_path = \"/kaggle/working/t5_chatbot_tokenizer\"\n\ntokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\nmodel.eval() \n\ndef generate_response(question):\n    input_ids = tokenizer(f\"question: {question} </s>\", return_tensors=\"pt\").input_ids.to(model.device)\n    outputs = model.generate(\n        input_ids,\n        max_length=128,\n        num_beams=5,  \n        no_repeat_ngram_size=2,  \n        top_k=50,  \n        top_p=0.95,  \n        temperature=1.0  \n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\nresponse = generate_response(\"What is Paget's Disease of Bone ?\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}