{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":9941146,"datasetId":6112205,"databundleVersionId":10202441},{"sourceType":"datasetVersion","sourceId":7639866,"datasetId":841565,"databundleVersionId":7736362},{"sourceType":"datasetVersion","sourceId":3723297,"datasetId":2222120,"databundleVersionId":3777646}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import torch\nimport wandb\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments\nfrom transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback, T5Config\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.amp import autocast  \n\nfrom collections import defaultdict\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-24T19:41:00.997775Z","iopub.execute_input":"2024-11-24T19:41:00.998426Z","iopub.status.idle":"2024-11-24T19:41:19.481979Z","shell.execute_reply.started":"2024-11-24T19:41:00.998386Z","shell.execute_reply":"2024-11-24T19:41:19.481285Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"code","source":"#Load Data\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\n# Sampel Data\nprint(\"Data Sample\")\nprint(df.head())\n\n#Null value\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\nduplicates = df.duplicated(['question'], keep=False).sum()\nprint(f\"Total duplicates in 'question' column: {duplicates}\")\n\n# Check for duplicate rows\n\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Reset the index after removing duplicates\ndf.reset_index(drop=True, inplace=True)\n\n#Delete Unused column\ndf = df.drop(columns=['source', 'focus_area'])\n\n#Table Info\nprint(\"Table Info\")\nprint(df.info())\n\n# Apply the function\ndf = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\ndf = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n\n#Drop rows with null values\ndf.dropna(inplace=True)\n\n#Checking again of null values\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n#Checking again of the data info\nprint(df.info())\n\n#Check for Unique Data\nprint(f\"Unique questions: {df['question'].nunique()}\")\nprint(f\"Unique answers: {df['answer'].nunique()}\")\n\ndf['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\ndf['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-24T19:41:19.483245Z","iopub.execute_input":"2024-11-24T19:41:19.483790Z","iopub.status.idle":"2024-11-24T19:41:21.015945Z","shell.execute_reply.started":"2024-11-24T19:41:19.483762Z","shell.execute_reply":"2024-11-24T19:41:21.014998Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  \nNull Value Data\nquestion       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64\nTotal duplicates in 'question' column: 2319\nNumber of duplicate rows: 48\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16364 entries, 0 to 16363\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  16364 non-null  object\n 1   answer    16359 non-null  object\ndtypes: object(2)\nmemory usage: 255.8+ KB\nNone\nNull Value Data\nquestion    0\nanswer      0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nIndex: 14463 entries, 0 to 14463\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  14463 non-null  object\n 1   answer    14463 non-null  object\ndtypes: object(2)\nmemory usage: 339.0+ KB\nNone\nUnique questions: 14463\nUnique answers: 14463\n                                 question  \\\n0                what is (are) glaucoma ?   \n1                  what causes glaucoma ?   \n2     what are the symptoms of glaucoma ?   \n3  what are the treatments for glaucoma ?   \n4          who is at risk for glaucoma? ?   \n\n                                              answer  \n0  glaucoma is a group of diseases that can damag...  \n1  nearly 2.7 million people have glaucoma, a lea...  \n2  symptoms of glaucoma glaucoma can develop in o...  \n3  although open-angle glaucoma cannot be cured, ...  \n4  anyone can develop glaucoma. some people are a...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Architecting Model","metadata":{}},{"cell_type":"code","source":"# Load T5-small model and tokenizer\nmodel_name = \"t5-small\"\nconfig = T5Config.from_pretrained(model_name)\nconfig.dropout_rate = 0.3\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Print model architecture summary\ndef print_model_summary(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"\\nModel Summary:\")\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n\nprint_model_summary(model)\n\n# Preprocess function for seq2seq task\ndef preprocess_function(batch):\n    inputs = [\n        f\"answer this question: {q} context: {c if 'context' in batch else ''} </s>\"\n        for q, c in zip(batch[\"question\"], batch.get(\"context\", [\"\"] * len(batch[\"question\"])))\n    ]\n    targets = [f\"{a} </s>\" for a in batch[\"answer\"]]\n\n    model_inputs = tokenizer(\n        inputs,\n        max_length=128,\n        truncation=True,\n        padding=\"longest\",\n        return_tensors=\"pt\",\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=64,\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n        )\n\n    # Replace padding tokens with -100 for loss computation\n    labels[\"input_ids\"][labels[\"input_ids\"] == tokenizer.pad_token_id] = -100\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Train-test split\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Preprocess datasets\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=1000,  \n    remove_columns=train_dataset.column_names,\n    num_proc=8,   \n)\n\nval_dataset = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=1000,  \n    remove_columns=val_dataset.column_names,\n    num_proc=8,  \n)\n\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,  \n    save_steps=500,  \n    save_total_limit=2,  \n    learning_rate=2e-4,   \n    num_train_epochs=10,   \n    per_device_train_batch_size=16,   \n    per_device_eval_batch_size=16, \n    lr_scheduler_type=\"cosine\",  \n    warmup_ratio=0.05,  \n    weight_decay=0.01,\n    predict_with_generate=True,\n    fp16=True,   \n    logging_dir=\"./logs\",\n    logging_steps=50,  \n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=2,   \n    max_grad_norm=1.0,\n    optim=\"adamw_torch_fused\",  \n    generation_max_length=64,  \n    generation_num_beams=5,\n    dataloader_num_workers=4,   \n    group_by_length=True, \n    remove_unused_columns=True,\n)\n\ntraining_args.label_smoothing_factor = 0.1\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,  \n    padding=True,  \n)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    # Handle padding and special tokens\n    predictions = np.where(predictions == -100, tokenizer.pad_token_id, predictions)\n    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n    \n    # Decode predictions and labels\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Clean and normalize text\n    def clean_text(text):\n        # Remove extra whitespace and normalize to lowercase\n        text = \" \".join(text.strip().lower().split())\n        return text\n    \n    decoded_preds = [clean_text(pred) for pred in decoded_preds]\n    decoded_labels = [clean_text(label) for label in decoded_labels]\n    \n    # Exact match accuracy\n    exact_matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n    exact_match_accuracy = np.mean(exact_matches) * 100\n    \n    # Word-level accuracy\n    def calculate_word_accuracy(pred, label):\n        pred_words = set(pred.split())\n        label_words = set(label.split())\n        if len(label_words) == 0:\n            return 0.0\n        return len(pred_words.intersection(label_words)) / len(label_words)\n    \n    word_accuracies = [calculate_word_accuracy(pred, label) \n                       for pred, label in zip(decoded_preds, decoded_labels)]\n    word_level_accuracy = np.mean(word_accuracies) * 100\n    \n    # Metrics dictionary\n    metrics = {\n        \"accuracy\": exact_match_accuracy,  \n        \"validation_accuracy\": word_level_accuracy,  \n    }\n    \n    # Log examples for debugging\n    if len(decoded_preds) > 0:\n        n_samples = min(3, len(decoded_preds))\n        for i in range(n_samples):\n            metrics[f\"example_{i+1}_pred\"] = decoded_preds[i][:100]\n            metrics[f\"example_{i+1}_true\"] = decoded_labels[i][:100]\n            metrics[f\"example_{i+1}_exact_match\"] = exact_matches[i]\n            metrics[f\"example_{i+1}_word_accuracy\"] = word_accuracies[i]\n    \n    return metrics\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding='longest',\n    return_tensors=\"pt\"\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer\ntrainer.save_model(\"./t5_chatbot_model\")\ntokenizer.save_pretrained(\"./t5_chatbot_tokenizer\")\nmodel_path = \"./t5_chatbot_model.h5\"\ntorch.save(model.state_dict(), model_path)\n\n# Save log history\nlog_history = trainer.state.log_history\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T19:41:21.017706Z","iopub.execute_input":"2024-11-24T19:41:21.018108Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da2b1b2e9f03442790b14e39916c3907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec761f562b9f4cf08d6beff5e917f34c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8af6802bd7034aa9b1a287ba00369faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2e132af02e4b54bef8c47655e6c2eb"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba744ff2bc43441f95326e7ee04a784a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc1945fb6cd466f8a81c210e5fb0700"}},"metadata":{}},{"name":"stdout","text":"\nModel Summary:\nTotal parameters: 60,506,624\nTrainable parameters: 60,506,624\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/13016 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de50418a5ff64833a1225c2d7c3e6990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/1447 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a112b93c9047d28cb4cb5d4d518ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1160' max='2030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1160/2030 12:49 < 09:38, 1.50 it/s, Epoch 5.70/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Validation Accuracy</th>\n      <th>Example 1 Pred</th>\n      <th>Example 1 True</th>\n      <th>Example 1 Exact Match</th>\n      <th>Example 1 Word Accuracy</th>\n      <th>Example 2 Pred</th>\n      <th>Example 2 True</th>\n      <th>Example 2 Exact Match</th>\n      <th>Example 2 Word Accuracy</th>\n      <th>Example 3 Pred</th>\n      <th>Example 3 True</th>\n      <th>Example 3 Exact Match</th>\n      <th>Example 3 Word Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.377900</td>\n      <td>3.283157</td>\n      <td>13.683483</td>\n      <td>37.599068</td>\n      <td>what are the signs and symptoms of charcot-marie-tooth disease type 2b? the human phenotype ontology</td>\n      <td>what are the signs and symptoms of charcot-marie-tooth disease type 2b? the human phenotype ontology</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>phosphoglycerate kinase deficiency is a condition that affects the body's immune system. it is a con</td>\n      <td>phosphoglycerate kinase deficiency is a genetic disorder that affects the body's ability to break do</td>\n      <td>False</td>\n      <td>0.243243</td>\n      <td>these resources address the diagnosis or management of beta thalassemia: - gene review: gene review:</td>\n      <td>these resources address the diagnosis or management of beta thalassemia: - gene review: gene review:</td>\n      <td>False</td>\n      <td>0.708333</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.257100</td>\n      <td>3.183666</td>\n      <td>13.545266</td>\n      <td>38.989594</td>\n      <td>what are the signs and symptoms of charcot-marie-tooth disease type 2b? the human phenotype ontology</td>\n      <td>what are the signs and symptoms of charcot-marie-tooth disease type 2b? the human phenotype ontology</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>phosphoglycerate kinase (pkinase) deficiency is a condition that affects the nervous system. it is c</td>\n      <td>phosphoglycerate kinase deficiency is a genetic disorder that affects the body's ability to break do</td>\n      <td>False</td>\n      <td>0.243243</td>\n      <td>these resources address the diagnosis or management of beta thalassemia: - gene review: gene review:</td>\n      <td>these resources address the diagnosis or management of beta thalassemia: - gene review: gene review:</td>\n      <td>False</td>\n      <td>0.708333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"steps = []\ntraining_loss = []\nvalidation_loss = []\naccuracy = []\nvalidation_accuracy = []\nprint(log_history)\n# Parse log history to extract relevant metrics\nfor entry in log_history:\n    if \"loss\" in entry:\n        steps.append(entry[\"step\"])\n        training_loss.append(entry[\"loss\"])\n    if \"eval_loss\" in entry:\n        validation_loss.append(entry[\"eval_loss\"])\n    if \"accuracy\" in entry:\n        accuracy.append(entry[\"accuracy\"])\n    if \"eval_validation_accuracy\" in entry:\n        validation_accuracy.append(entry[\"eval_validation_accuracy\"])\n\n# Plot metrics\nplt.figure(figsize=(14, 8))\n\n# Subplot 1: Loss vs Validation Loss\nplt.subplot(2, 1, 1)\nplt.plot(steps[:len(training_loss)], training_loss, label=\"Training Loss\", marker=\"o\", linestyle=\"-\")\nif validation_loss:\n    plt.plot(steps[:len(validation_loss)], validation_loss, label=\"Validation Loss\", marker=\"x\", linestyle=\"--\")\nplt.title(\"Loss and Validation Loss\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\n# Subplot 2: Accuracy vs Validation Accuracy\nplt.subplot(2, 1, 2)\nif accuracy:\n    plt.plot(steps[:len(accuracy)], accuracy, label=\"Training Accuracy\", marker=\"o\", linestyle=\"-\")\nif validation_accuracy:\n    plt.plot(steps[:len(validation_accuracy)], validation_accuracy, label=\"Validation Accuracy\", marker=\"x\", linestyle=\"--\")\nplt.title(\"Accuracy and Validation Accuracy\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.grid(True)\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing using Input","metadata":{}},{"cell_type":"code","source":"# Load the trained T5 model and tokenizer\nmodel_path = \"/kaggle/working/t5_chatbot_model\"\ntokenizer_path = \"/kaggle/working/t5_chatbot_tokenizer\"\n\ntokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\nmodel.eval() \n\ndef generate_response(question):\n    input_ids = tokenizer(f\"question: {question} </s>\", return_tensors=\"pt\").input_ids.to(model.device)\n    outputs = model.generate(\n        input_ids,\n        max_length=128,\n        num_beams=5,  \n        no_repeat_ngram_size=2,  \n        top_k=50,  \n        top_p=0.95,  \n        temperature=1.0  \n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\nresponse = generate_response(\"What is glaucoma ?\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}